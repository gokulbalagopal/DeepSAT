{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14231,"sourceType":"datasetVersion","datasetId":9970},{"sourceId":8572381,"sourceType":"datasetVersion","datasetId":5125892}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-31T21:43:46.594982Z","iopub.execute_input":"2024-05-31T21:43:46.595392Z","iopub.status.idle":"2024-05-31T21:43:46.600220Z","shell.execute_reply.started":"2024-05-31T21:43:46.595353Z","shell.execute_reply":"2024-05-31T21:43:46.599384Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":" !conda install -c conda-forge vit-pytorch -y","metadata":{"execution":{"iopub.status.busy":"2024-05-31T21:51:35.689022Z","iopub.execute_input":"2024-05-31T21:51:35.689921Z","iopub.status.idle":"2024-05-31T21:54:35.975007Z","shell.execute_reply.started":"2024-05-31T21:51:35.689890Z","shell.execute_reply":"2024-05-31T21:54:35.973961Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - conda-forge\n - rapidsai\n - nvidia\n - defaults\n - pytorch\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - vit-pytorch\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    blas-1.1                   |         openblas           1 KB  conda-forge\n    einops-0.8.0               |     pyhd8ed1ab_0          39 KB  conda-forge\n    filelock-3.14.0            |     pyhd8ed1ab_0          16 KB  conda-forge\n    gmpy2-2.1.5                |  py310hc7909c9_1         201 KB  conda-forge\n    libtorch-2.1.2             |cpu_generic_h7795521_0        45.3 MB  conda-forge\n    mpc-1.3.1                  |       hfe3b2da_0         114 KB  conda-forge\n    mpfr-4.2.1                 |       h9458935_1         628 KB  conda-forge\n    mpmath-1.3.0               |     pyhd8ed1ab_0         428 KB  conda-forge\n    networkx-3.3               |     pyhd8ed1ab_1         1.1 MB  conda-forge\n    nomkl-3.0                  |                0          46 KB\n    openblas-0.3.27            |pthreads_h7a3da1a_0         5.5 MB  conda-forge\n    pytorch-2.1.2              |cpu_generic_py310h730b6a9_0        25.4 MB  conda-forge\n    sleef-3.5.1                |       h9b69904_2         1.5 MB  conda-forge\n    sympy-1.12                 | pypyh9d50eac_103         4.1 MB  conda-forge\n    torchvision-0.16.1         |cpu_py310h684a773_3         9.7 MB  conda-forge\n    vit-pytorch-1.6.8          |     pyhd8ed1ab_0          64 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        94.1 MB\n\nThe following NEW packages will be INSTALLED:\n\n  blas               conda-forge/linux-64::blas-1.1-openblas \n  einops             conda-forge/noarch::einops-0.8.0-pyhd8ed1ab_0 \n  filelock           conda-forge/noarch::filelock-3.14.0-pyhd8ed1ab_0 \n  gmpy2              conda-forge/linux-64::gmpy2-2.1.5-py310hc7909c9_1 \n  libtorch           conda-forge/linux-64::libtorch-2.1.2-cpu_generic_h7795521_0 \n  mpc                conda-forge/linux-64::mpc-1.3.1-hfe3b2da_0 \n  mpfr               conda-forge/linux-64::mpfr-4.2.1-h9458935_1 \n  mpmath             conda-forge/noarch::mpmath-1.3.0-pyhd8ed1ab_0 \n  networkx           conda-forge/noarch::networkx-3.3-pyhd8ed1ab_1 \n  nomkl              pkgs/main/linux-64::nomkl-3.0-0 \n  openblas           conda-forge/linux-64::openblas-0.3.27-pthreads_h7a3da1a_0 \n  pytorch            conda-forge/linux-64::pytorch-2.1.2-cpu_generic_py310h730b6a9_0 \n  sleef              conda-forge/linux-64::sleef-3.5.1-h9b69904_2 \n  sympy              conda-forge/noarch::sympy-1.12-pypyh9d50eac_103 \n  torchvision        conda-forge/linux-64::torchvision-0.16.1-cpu_py310h684a773_3 \n  vit-pytorch        conda-forge/noarch::vit-pytorch-1.6.8-pyhd8ed1ab_0 \n\n\n\nDownloading and Extracting Packages:\nlibtorch-2.1.2       | 45.3 MB   |                                       |   0% \npytorch-2.1.2        | 25.4 MB   |                                       |   0% \u001b[A\n\ntorchvision-0.16.1   | 9.7 MB    |                                       |   0% \u001b[A\u001b[A\n\n\nopenblas-0.3.27      | 5.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nsympy-1.12           | 4.1 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nsleef-3.5.1          | 1.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nnetworkx-3.3         | 1.1 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nmpfr-4.2.1           | 628 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nmpmath-1.3.0         | 428 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ngmpy2-2.1.5          | 201 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nmpc-1.3.1            | 114 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nvit-pytorch-1.6.8    | 64 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nnomkl-3.0            | 46 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\neinops-0.8.0         | 39 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nfilelock-3.14.0      | 16 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibtorch-2.1.2       | 45.3 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-2.1.2        | 25.4 MB   |                                       |   0% \u001b[A\n\n\nopenblas-0.3.27      | 5.5 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\n\ntorchvision-0.16.1   | 9.7 MB    |                                       |   0% \u001b[A\u001b[A\n\n\n\nlibtorch-2.1.2       | 45.3 MB   | ##                                    |   6% \u001b[A\u001b[A\u001b[A\u001b[A\npytorch-2.1.2        | 25.4 MB   | ###5                                  |   9% \u001b[A\n\n\nopenblas-0.3.27      | 5.5 MB    | #################7                    |  48% \u001b[A\u001b[A\u001b[A\n\ntorchvision-0.16.1   | 9.7 MB    | ########2                             |  22% \u001b[A\u001b[A\n\n\n\nlibtorch-2.1.2       | 45.3 MB   | ####4                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\npytorch-2.1.2        | 25.4 MB   | #######7                              |  21% \u001b[A\n\n\nopenblas-0.3.27      | 5.5 MB    | ####################################6 |  99% \u001b[A\u001b[A\u001b[A\n\nlibtorch-2.1.2       | 45.3 MB   | #######6                              |  21% \u001b[A\u001b[A\npytorch-2.1.2        | 25.4 MB   | #############5                        |  37% \u001b[A\n\ntorchvision-0.16.1   | 9.7 MB    | #################################8    |  91% \u001b[A\u001b[A\n\n\n\n\nlibtorch-2.1.2       | 45.3 MB   | ##########1                           |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\npytorch-2.1.2        | 25.4 MB   | #################8                    |  48% \u001b[A\n\n\n\n\n\nnetworkx-3.3         | 1.1 MB    | 5                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nlibtorch-2.1.2       | 45.3 MB   | ############5                         |  34% \u001b[A\n\n\n\n\n\n\nmpfr-4.2.1           | 628 KB    | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nmpmath-1.3.0         | 428 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ngmpy2-2.1.5          | 201 KB    | ##9                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nlibtorch-2.1.2       | 45.3 MB   | ##############8                       |  40% \u001b[A\n\n\n\n\n\n\n\n\n\nmpc-1.3.1            | 114 KB    | #####2                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nvit-pytorch-1.6.8    | 64 KB     | #########2                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nlibtorch-2.1.2       | 45.3 MB   | #################                     |  46% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\nnomkl-3.0            | 46 KB     | ############9                         |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\neinops-0.8.0         | 39 KB     | ###############                       |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nfilelock-3.14.0      | 16 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblas-1.1             | 1 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nlibtorch-2.1.2       | 45.3 MB   | ##################################    |  92% \u001b[A\n\n\n\n\nsleef-3.5.1          | 1.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nsleef-3.5.1          | 1.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nopenblas-0.3.27      | 5.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nmpfr-4.2.1           | 628 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nmpfr-4.2.1           | 628 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nmpmath-1.3.0         | 428 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nmpmath-1.3.0         | 428 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ngmpy2-2.1.5          | 201 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ngmpy2-2.1.5          | 201 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nmpc-1.3.1            | 114 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nmpc-1.3.1            | 114 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nvit-pytorch-1.6.8    | 64 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nvit-pytorch-1.6.8    | 64 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nnetworkx-3.3         | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nnetworkx-3.3         | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nnomkl-3.0            | 46 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nnomkl-3.0            | 46 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\neinops-0.8.0         | 39 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\neinops-0.8.0         | 39 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nfilelock-3.14.0      | 16 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblas-1.1             | 1 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nsympy-1.12           | 4.1 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nsympy-1.12           | 4.1 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\ntorchvision-0.16.1   | 9.7 MB    | ##################################### | 100% \u001b[A\u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}]},{"cell_type":"code","source":"################### Classical ML Models ######################\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage.io import imshow\nimport scipy\nimport psutil\nimport time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Load and preprocess data\ndef image_processing(df_x):\n    reshaped_x = df_x.values.reshape(-1, 28, 28, 4).astype(float)\n    reshaped_x_new = reshaped_x / 255.0\n\n    x_grey = []\n    # Convert images to gray scale\n    for i in range(len(reshaped_x_new)):\n        grey_image = np.dot(reshaped_x_new[i, :, :, 0:3], [0.2989, 0.5870, 0.1140])\n        x_grey.append(grey_image)\n\n    x_grey_np = np.asarray(x_grey)\n    reshaped_x_grey = x_grey_np.reshape(-1, 28, 28, 1)\n    svd_feat = []\n    for i in range(len(reshaped_x_grey)):\n        temp = np.asmatrix(reshaped_x_grey[i, :, :, :])\n        svd_feat.append(scipy.linalg.svdvals(temp))\n\n    # Plot SVD features\n    svd_arr = np.array(svd_feat)\n    plt.plot(range(28), svd_feat[1], \"-ok\")\n    plt.xlabel('Number')\n    plt.ylabel('Features')\n    plt.title('Number vs features')\n    plt.show()\n    \n    # Compute RGB channel means\n    r_mean = []\n    g_mean = []\n    b_mean = []\n\n    for i in range(len(reshaped_x_new)):\n        r_mean.append(np.mean(reshaped_x_new[i, :, :, 0]))\n        g_mean.append(np.mean(reshaped_x_new[i, :, :, 1]))\n        b_mean.append(np.mean(reshaped_x_new[i, :, :, 2]))\n\n    df_means = pd.DataFrame(list(zip(r_mean, g_mean, b_mean)), columns=['R Mean', 'G Mean', 'B Mean'])\n    svd_feat_np = np.asarray(svd_feat)\n    df_svd_feat = pd.DataFrame(svd_feat_np)\n\n    # Combine SVD features and channel means\n    df_X = pd.concat([df_svd_feat, df_means], axis=1)\n    df_X.columns = df_X.columns.astype(str)\n    initial_len = len(df_X)\n    df_X = df_X.dropna()\n    return df_X\n\n\ndef label_processing(df_y):\n    df_y['Labels'] = \"NA\"\n    for ix in range(len(df_y)):\n        if df_y.iloc[ix, 0] == 1:\n            df_y.iloc[ix, 4] = \"Barren Land\"\n        elif df_y.iloc[ix, 1] == 1:\n            df_y.iloc[ix, 4] = \"Trees\"\n        elif df_y.iloc[ix, 2] == 1:\n            df_y.iloc[ix, 4] = \"Grassland\"\n        else:\n            df_y.iloc[ix, 4] = \"None\"\n    df_y = df_y['Labels']\n    label_map = {\"Barren Land\": 0, \"Trees\": 1, \"Grassland\": 2, \"None\": 3}\n    labels = df_y.map(label_map).values\n    return labels\n\n\n\npath = \"/kaggle/input/deepsat4-subsets/\"\n\ndf_X_train = pd.read_csv(path + \"chunk_x_train_1.csv\")\ndf_y_train = pd.read_csv(path + \"chunk_y_train_1.csv\")\ndf_X_test = pd.read_csv(path + \"chunk_x_test_1.csv\")\ndf_y_test = pd.read_csv(path + \"chunk_y_test_1.csv\")\n\n\n# Drop rows with missing values for training set\ndf_X_train = df_X_train.dropna()\ndf_y_train = df_y_train.iloc[df_X_train.index]\n\n# Drop rows with missing values for test set\ndf_X_test = df_X_test.dropna()\ndf_y_test = df_y_test.iloc[df_X_test.index]\n\nreshaped_x_train = image_processing(df_X_train)\ntrain_labels = label_processing(df_y_train)\n\nreshaped_x_test = image_processing(df_X_test)\ntest_labels = label_processing(df_y_test)\n\n# Ensure the labels are in 1D array format\ntrain_labels = train_labels.flatten()\ntest_labels = test_labels.flatten()\n\n\n# Initialize classifiers\nclassifiers = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n    'Gaussian Naive Bayes': GaussianNB(),\n    'Support Vector Machine': SVC(),\n    'Random Forest': RandomForestClassifier(n_estimators=50)\n}\n\n# Start time measurement\nstart_time = time.time()\n\n# Train and evaluate classifiers\nbest_model = None\nbest_score = 0\nbest_model_name = \"\"\nfor name, clf in classifiers.items():\n    clf.fit(reshaped_x_train, train_labels)\n    train_score = clf.score(reshaped_x_train, train_labels)\n    test_score = clf.score(reshaped_x_test, test_labels)\n    print(f'Accuracy of {name} on training set: {train_score:.2f}')\n    print(f'Accuracy of {name} on test set: {test_score:.2f}')\n    if test_score > best_score:\n        best_score = test_score\n        best_model = clf\n        best_model_name = name\n\nend_time = time.time()\ntime_taken = end_time - start_time\n\n# Measure resource usage\nprocess = psutil.Process()\nmemory_info = process.memory_info()\ncpu_times = process.cpu_times()\n\nprint(f'Time taken to find the best model: {time_taken:.2f} seconds')\nprint(f'Memory usage: {memory_info.rss / (1024 ** 2):.2f} MB')\nprint(f'CPU time: {cpu_times.user + cpu_times.system:.2f} seconds')\n\nprint(f'Best model: {best_model_name} with accuracy: {best_score:.2f}')\n\n# Evaluate the best model\npred = best_model.predict(reshaped_x_test)\nprint(confusion_matrix(test_labels, pred))\nprint(classification_report(test_labels, pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T22:27:18.443443Z","iopub.execute_input":"2024-05-31T22:27:18.443762Z","iopub.status.idle":"2024-05-31T22:31:12.871514Z","shell.execute_reply.started":"2024-05-31T22:27:18.443736Z","shell.execute_reply":"2024-05-31T22:31:12.870563Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of Logistic Regression on training set: 0.82\nAccuracy of Logistic Regression on test set: 0.82\nAccuracy of Decision Tree on training set: 1.00\nAccuracy of Decision Tree on test set: 0.94\nAccuracy of K-Nearest Neighbors on training set: 0.93\nAccuracy of K-Nearest Neighbors on test set: 0.90\nAccuracy of Linear Discriminant Analysis on training set: 0.84\nAccuracy of Linear Discriminant Analysis on test set: 0.83\nAccuracy of Gaussian Naive Bayes on training set: 0.68\nAccuracy of Gaussian Naive Bayes on test set: 0.69\nAccuracy of Support Vector Machine on training set: 0.85\nAccuracy of Support Vector Machine on test set: 0.85\nAccuracy of Random Forest on training set: 1.00\nAccuracy of Random Forest on test set: 0.96\nTime taken to find the best model: 182.99 seconds\nMemory usage: 6598.69 MB\nCPU time: 3127.82 seconds\nBest model: Random Forest with accuracy: 0.96\n[[2570    4   44   37]\n [   0 1934   11   69]\n [  38   23 1706   83]\n [  59   42   36 3344]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97      2655\n           1       0.97      0.96      0.96      2014\n           2       0.95      0.92      0.94      1850\n           3       0.95      0.96      0.95      3481\n\n    accuracy                           0.96     10000\n   macro avg       0.96      0.95      0.95     10000\nweighted avg       0.96      0.96      0.96     10000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#################### ViT ####################\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom vit_pytorch import ViT\nimport time\nimport psutil\n\n# Define custom dataset\nclass Sat4Dataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].astype(np.float32)\n        label = self.labels[idx].astype(np.int64)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n    \n# Load and preprocess data\ndef image_processing(df_x):\n    reshaped_X = df_x.values.reshape(-1, 28, 28, 4).astype(float)\n    reshaped_X_new = reshaped_X / 255.0\n    reshaped_X_rgb = reshaped_X_new[:, :, :, :3]\n    return reshaped_X_rgb\n\ndef label_processing(df_y):\n    df_y['Labels'] = \"NA\"\n    for ix in range(len(df_y)):\n        if df_y.iloc[ix, 0] == 1:\n            df_y.iloc[ix, 4] = \"Barren Land\"\n        elif df_y.iloc[ix, 1] == 1:\n            df_y.iloc[ix, 4] = \"Trees\"\n        elif df_y.iloc[ix, 2] == 1:\n            df_y.iloc[ix, 4] = \"Grassland\"\n        else:\n            df_y.iloc[ix, 4] = \"None\"\n    df_y = df_y['Labels']\n    label_map = {\"Barren Land\": 0, \"Trees\": 1, \"Grassland\": 2, \"None\": 3}\n    labels = df_y.map(label_map).values\n    return labels\n\npath = \"/kaggle/input/deepsat4-subsets/\"\n\ndf_x_train = pd.read_csv(path + \"chunk_x_train_1.csv\")\ndf_y_train = pd.read_csv(path + \"chunk_y_train_1.csv\")\ndf_x_test = pd.read_csv(path + \"chunk_x_test_1.csv\")\ndf_y_test = pd.read_csv(path + \"chunk_y_test_1.csv\")\n\nreshaped_x_train_rgb = image_processing(df_x_train)\ntrain_labels = label_processing(df_y_train)\n\nreshaped_x_test_rgb = image_processing(df_x_test)\ntest_labels = label_processing(df_y_test)\n\nprint(\"Unique train labels:\", np.unique(train_labels))\nprint(\"Unique test labels:\", np.unique(test_labels))\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = Sat4Dataset(reshaped_x_train_rgb, train_labels, transform)\ntest_dataset = Sat4Dataset(reshaped_x_test_rgb, test_labels, transform)\ntrain_size = len(train_dataset)\ntest_size = len(test_dataset)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Check if GPU is available and set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# Initialize ViT model, loss function, and optimizer\nmodel = ViT(\n    image_size = 224,\n    patch_size = 32,\n    num_classes = 4,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Measure start time and resource usage\nstart_time = time.time()\nprocess = psutil.Process()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n\n# Measure end time and resource usage\nend_time = time.time()\ntime_taken = end_time - start_time\nmemory_info = process.memory_info()\ncpu_times = process.cpu_times()\n\nprint(f'Time taken to train the model: {time_taken:.2f} seconds')\nprint(f'Memory usage: {memory_info.rss / (1024 ** 2):.2f} MB')\nprint(f'CPU time: {cpu_times.user + cpu_times.system:.2f} seconds')\n\n# Evaluation\nmodel.eval()\ny_true = []\ny_pred = []\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n# Confusion matrix and classification report\nprint(confusion_matrix(y_true, y_pred))\nprint(classification_report(y_true, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T21:54:35.976932Z","iopub.execute_input":"2024-05-31T21:54:35.977242Z","iopub.status.idle":"2024-05-31T22:27:18.441494Z","shell.execute_reply.started":"2024-05-31T21:54:35.977211Z","shell.execute_reply":"2024-05-31T22:27:18.440486Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Unique train labels: [0 1 2 3]\nUnique test labels: [0 1 2 3]\ncuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.8519095879457795\nEpoch 2/10, Loss: 0.7385933524964715\nEpoch 3/10, Loss: 0.7487748293849208\nEpoch 4/10, Loss: 0.7298540312158482\nEpoch 5/10, Loss: 0.7249136074729587\nEpoch 6/10, Loss: 0.7151487050458903\nEpoch 7/10, Loss: 0.6959304028688489\nEpoch 8/10, Loss: 0.7374428406624538\nEpoch 9/10, Loss: 0.7227508773660416\nEpoch 10/10, Loss: 0.7359507107140159\nTime taken to train the model: 1901.82 seconds\nMemory usage: 6439.40 MB\nCPU time: 2836.96 seconds\n[[2498    6   38  113]\n [   5 1577   11  421]\n [1130   51  276  393]\n [ 157 1214   60 2050]]\n              precision    recall  f1-score   support\n\n           0       0.66      0.94      0.78      2655\n           1       0.55      0.78      0.65      2014\n           2       0.72      0.15      0.25      1850\n           3       0.69      0.59      0.63      3481\n\n    accuracy                           0.64     10000\n   macro avg       0.65      0.62      0.58     10000\nweighted avg       0.66      0.64      0.60     10000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"################## ResNet ######################\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import transforms, models\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport psutil\nimport time\n\n\n# Define custom dataset\nclass Sat4Dataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].astype(np.float32)\n        label = self.labels[idx].astype(np.int64)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Load pre-trained ResNet and modify it for our task\nclass ResNetModel(nn.Module):\n    def __init__(self):\n        super(ResNetModel, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 4)\n    \n    def forward(self, x):\n        return self.resnet(x)\n\n\ndef image_processing(df_x):\n    reshaped_X = df_x.values.reshape(-1, 28, 28, 4).astype(float)\n    reshaped_X_new = reshaped_X / 255.0\n    reshaped_X_rgb = reshaped_X_new[:, :, :, :3]\n    return reshaped_X_rgb\n\ndef label_processing(df_y):\n    df_y['Labels'] = \"NA\"\n    for ix in range(len(df_y)):\n        if df_y.iloc[ix, 0] == 1:\n            df_y.iloc[ix, 4] = \"Barren Land\"\n        elif df_y.iloc[ix, 1] == 1:\n            df_y.iloc[ix, 4] = \"Trees\"\n        elif df_y.iloc[ix, 2] == 1:\n            df_y.iloc[ix, 4] = \"Grassland\"\n        else:\n            df_y.iloc[ix, 4] = \"None\"\n    df_y = df_y['Labels']\n    label_map = {\"Barren Land\": 0, \"Trees\": 1, \"Grassland\": 2, \"None\": 3}\n    labels = df_y.map(label_map).values\n    return labels\n\npath = \"/kaggle/input/deepsat4-subsets/\"\n\ndf_x_train = pd.read_csv(path + \"chunk_x_train_1.csv\")\ndf_y_train = pd.read_csv(path + \"chunk_y_train_1.csv\")\ndf_x_test = pd.read_csv(path + \"chunk_x_test_1.csv\")\ndf_y_test = pd.read_csv(path + \"chunk_y_test_1.csv\")\n\nreshaped_x_train_rgb = image_processing(df_x_train)\ntrain_labels = label_processing(df_y_train)\n\nreshaped_x_test_rgb = image_processing(df_x_test)\ntest_labels = label_processing(df_y_test)\n\nprint(\"Unique train labels:\", np.unique(train_labels))\nprint(\"Unique test labels:\", np.unique(test_labels))\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = Sat4Dataset(reshaped_x_train_rgb, train_labels, transform)\ntest_dataset = Sat4Dataset(reshaped_x_test_rgb, test_labels, transform)\ntrain_size = len(train_dataset)\ntest_size = len(test_dataset)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Check if GPU is available and set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# Initialize model, loss function, and optimizer\nmodel = ResNetModel().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Measure start time and resource usage\nstart_time = time.time()\nprocess = psutil.Process()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n\n# Measure end time and resource usage\nend_time = time.time()\ntime_taken = end_time - start_time\nmemory_info = process.memory_info()\ncpu_times = process.cpu_times()\n\nprint(f'Time taken to train the model: {time_taken:.2f} seconds')\nprint(f'Memory usage: {memory_info.rss / (1024 ** 2):.2f} MB')\nprint(f'CPU time: {cpu_times.user + cpu_times.system:.2f} seconds')\n\n# Evaluation\nmodel.eval()\ny_true = []\ny_pred = []\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n# Confusion matrix and classification report\nprint(confusion_matrix(y_true, y_pred))\nprint(classification_report(y_true, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T22:31:12.872674Z","iopub.execute_input":"2024-05-31T22:31:12.872997Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Unique train labels: [0 1 2 3]\nUnique test labels: [0 1 2 3]\ncuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 163MB/s]\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.09520092789280822\nEpoch 2/10, Loss: 0.04846923733847645\nEpoch 3/10, Loss: 0.03710568104410613\nEpoch 4/10, Loss: 0.03626717884576453\nEpoch 5/10, Loss: 0.0234782246443036\nEpoch 6/10, Loss: 0.020115810567773815\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport psutil\nimport time\nimport scipy.linalg\n\n# Define custom dataset\nclass Sat4Dataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].astype(np.float32)\n        label = self.labels[idx].astype(np.int64)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Load pre-trained ResNet and modify it for our task\nclass ResNetModel(nn.Module):\n    def __init__(self):\n        super(ResNetModel, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 4)\n    \n    def forward(self, x):\n        return self.resnet(x)\n\n# Define ViT model\nfrom vit_pytorch import ViT\n\nclass ViTModel(nn.Module):\n    def __init__(self):\n        super(ViTModel, self).__init__()\n        self.vit = ViT(\n            image_size = 224,\n            patch_size = 32,\n            num_classes = 4,\n            dim = 1024,\n            depth = 6,\n            heads = 16,\n            mlp_dim = 2048,\n            dropout = 0.1,\n            emb_dropout = 0.1\n        )\n    \n    def forward(self, x):\n        return self.vit(x)\n\ndef image_processing_regular_classification(df_x):\n    reshaped_x = df_x.values.reshape(-1, 28, 28, 4).astype(float)\n    reshaped_x_new = reshaped_x / 255.0\n\n    x_grey = []\n    # Convert images to gray scale\n    for i in range(len(reshaped_x_new)):\n        grey_image = np.dot(reshaped_x_new[i, :, :, 0:3], [0.2989, 0.5870, 0.1140])\n        x_grey.append(grey_image)\n\n    x_grey_np = np.asarray(x_grey)\n    reshaped_x_grey = x_grey_np.reshape(-1, 28, 28, 1)\n    svd_feat = []\n    for i in range(len(reshaped_x_grey)):\n        temp = np.asmatrix(reshaped_x_grey[i, :, :, :])\n        svd_feat.append(scipy.linalg.svdvals(temp))\n\n    # Compute RGB channel means\n    r_mean = []\n    g_mean = []\n    b_mean = []\n\n    for i in range(len(reshaped_x_new)):\n        r_mean.append(np.mean(reshaped_x_new[i, :, :, 0]))\n        g_mean.append(np.mean(reshaped_x_new[i, :, :, 1]))\n        b_mean.append(np.mean(reshaped_x_new[i, :, :, 2]))\n\n    df_means = pd.DataFrame(list(zip(r_mean, g_mean, b_mean)), columns=['R Mean', 'G Mean', 'B Mean'])\n    svd_feat_np = np.asarray(svd_feat)\n    df_svd_feat = pd.DataFrame(svd_feat_np)\n\n    # Combine SVD features and channel means\n    df_X = pd.concat([df_svd_feat, df_means], axis=1)\n    df_X.columns = df_X.columns.astype(str)\n    initial_len = len(df_X)\n    df_X = df_X.dropna()\n    return df_X\n\ndef image_processing(df_x):\n    reshaped_X = df_x.values.reshape(-1, 28, 28, 4).astype(float)\n    reshaped_X_new = reshaped_X / 255.0\n    reshaped_X_rgb = reshaped_X_new[:, :, :, :3]\n    return reshaped_X_rgb\n\ndef label_processing(df_y):\n    df_y['Labels'] = \"NA\"\n    for ix in range(len(df_y)):\n        if df_y.iloc[ix, 0] == 1:\n            df_y.iloc[ix, 4] = \"Barren Land\"\n        elif df_y.iloc[ix, 1] == 1:\n            df_y.iloc[ix, 4] = \"Trees\"\n        elif df_y.iloc[ix, 2] == 1:\n            df_y.iloc[ix, 4] = \"Grassland\"\n        else:\n            df_y.iloc[ix, 4] = \"None\"\n    df_y = df_y['Labels']\n    label_map = {\"Barren Land\": 0, \"Trees\": 1, \"Grassland\": 2, \"None\": 3}\n    labels = df_y.map(label_map).values\n    return labels\n\npath = \"/kaggle/input/deepsat4-subsets/\"\n\ndf_x_train = pd.read_csv(path + \"chunk_x_train_1.csv\")\ndf_y_train = pd.read_csv(path + \"chunk_y_train_1.csv\")\ndf_x_test = pd.read_csv(path + \"chunk_x_test_1.csv\")\ndf_y_test = pd.read_csv(path + \"chunk_y_test_1.csv\")\n\ntrain_labels = label_processing(df_y_train)\ntest_labels = label_processing(df_y_test)\n\n# Data preprocessing for regular classifiers\nreshaped_x_train_regular = image_processing_regular_classification(df_x_train)\nreshaped_x_test_regular = image_processing_regular_classification(df_x_test)\n\n# Data preprocessing for ViT and ResNet\nreshaped_x_train_rgb = image_processing(df_x_train)\nreshaped_x_test_rgb = image_processing(df_x_test)\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = Sat4Dataset(reshaped_x_train_rgb, train_labels, transform)\ntest_dataset = Sat4Dataset(reshaped_x_test_rgb, test_labels, transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Check if GPU is available and set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndef train_and_evaluate_model(model, train_loader, test_loader, num_epochs=10):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    start_time = time.time()\n    process = psutil.Process()\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n    \n    end_time = time.time()\n    time_taken = end_time - start_time\n    memory_info = process.memory_info()\n    cpu_times = process.cpu_times()\n    \n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n\n    return {\n        \"time_taken\": time_taken,\n        \"memory_usage\": memory_info.rss / (1024 ** 2),\n        \"cpu_time\": cpu_times.user + cpu_times.system,\n        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n        \"classification_report\": classification_report(y_true, y_pred, output_dict=True)\n    }\n\n# Regular classifiers\ndef train_and_evaluate_classifiers(X_train, y_train, X_test, y_test):\n    classifiers = {\n        'Logistic Regression': LogisticRegression(),\n        'Decision Tree': DecisionTreeClassifier(),\n        'K-Nearest Neighbors': KNeighborsClassifier(),\n        'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n        'Gaussian Naive Bayes': GaussianNB(),\n        'Support Vector Machine': SVC(),\n        'Random Forest': RandomForestClassifier(n_estimators=50)\n    }\n\n    start_time = time.time()\n    process = psutil.Process()\n    \n    best_model = None\n    best_score = 0\n    best_model_name = \"\"\n    for name, clf in classifiers.items():\n        clf.fit(X_train, y_train)\n        test_score = clf.score(X_test, y_test)\n        if test_score > best_score:\n            best_score = test_score\n            best_model = clf\n            best_model_name = name\n    \n    end_time = time.time()\n    time_taken = end_time - start_time\n    memory_info = process.memory_info()\n    cpu_times = process.cpu_times()\n    \n    y_pred = best_model.predict(X_test)\n    \n    return {\n        \"time_taken\": time_taken,\n        \"memory_usage\": memory_info.rss / (1024 ** 2),\n        \"cpu_time\": cpu_times.user + cpu_times.system,\n        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n        \"classification_report\": classification_report(y_test, y_pred, output_dict=True),\n        \"best_model_name\": best_model_name,\n        \"best_model_score\": best_score\n    }\n\n# Evaluate regular classifiers\nresults_regular = train_and_evaluate_classifiers(reshaped_x_train_regular, train_labels, reshaped_x_test_regular, test_labels)\n\n# Evaluate ViT model\nvit_model = ViTModel().to(device)\nresults_vit = train_and_evaluate_model(vit_model, train_loader, test_loader)\n\n# Evaluate ResNet model\nresnet_model = ResNetModel().to(device)\nresults_resnet = train_and_evaluate_model(resnet_model, train_loader, test_loader)\n\n# Print results\nprint(\"Regular Classifiers:\")\nprint(f\"Best Model: {results_regular['best_model_name']} with accuracy: {results_regular['best_model_score']:.2f}\")\nprint(f\"Time taken: {results_regular['time_taken']:.2f} seconds\")\nprint(f\"Memory usage: {results_regular['memory_usage']:.2f} MB\")\nprint(f\"CPU time: {results_regular['cpu_time']:.2f} seconds\")\nprint(\"Confusion Matrix:\")\nprint(results_regular['confusion_matrix'])\nprint(\"Classification Report:\")\nprint(pd.DataFrame(results_regular['classification_report']).transpose())\n\nprint(\"\\nViT Model:\")\nprint(f\"Time taken: {results_vit['time_taken']:.2f} seconds\")\nprint(f\"Memory usage: {results_vit['memory_usage']:.2f} MB\")\nprint(f\"CPU time: {results_vit['cpu_time']:.2f} seconds\")\nprint(\"Confusion Matrix:\")\nprint(results_vit['confusion_matrix'])\nprint(\"Classification Report:\")\nprint(pd.DataFrame(results_vit['classification_report']).transpose())\n\nprint(\"\\nResNet Model:\")\nprint(f\"Time taken: {results_resnet['time_taken']:.2f} seconds\")\nprint(f\"Memory usage: {results_resnet['memory_usage']:.2f} MB\")\nprint(f\"CPU time: {results_resnet['cpu_time']:.2f} seconds\")\nprint(\"Confusion Matrix:\")\nprint(results_resnet['confusion_matrix'])\nprint(\"Classification Report:\")\nprint(pd.DataFrame(results_resnet['classification_report']).transpose())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport psutil\nimport time\nimport scipy.linalg\n\n# Define custom dataset\nclass Sat4Dataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].astype(np.float32)\n        label = self.labels[idx].astype(np.int64)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Load pre-trained ResNet and modify it for our task\nclass ResNetModel(nn.Module):\n    def __init__(self):\n        super(ResNetModel, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 4)\n    \n    def forward(self, x):\n        return self.resnet(x)\n\n# Define ViT model\nfrom vit_pytorch import ViT\n\nclass ViTModel(nn.Module):\n    def __init__(self):\n        super(ViTModel, self).__init__()\n        self.vit = ViT(\n            image_size = 224,\n            patch_size = 32,\n            num_classes = 4,\n            dim = 1024,\n            depth = 6,\n            heads = 16,\n            mlp_dim = 2048,\n            dropout = 0.1,\n            emb_dropout = 0.1\n        )\n    \n    def forward(self, x):\n        return self.vit(x)\n\ndef image_processing_regular_classification(df_x):\n    reshaped_x = df_x.values.reshape(-1, 28, 28, 4).astype(float)\n    reshaped_x_new = reshaped_x / 255.0\n\n    x_grey = []\n    # Convert images to gray scale\n    for i in range(len(reshaped_x_new)):\n        grey_image = np.dot(reshaped_x_new[i, :, :, 0:3], [0.2989, 0.5870, 0.1140])\n        x_grey.append(grey_image)\n\n    x_grey_np = np.asarray(x_grey)\n    reshaped_x_grey = x_grey_np.reshape(-1, 28, 28, 1)\n    svd_feat = []\n    for i in range(len(reshaped_x_grey)):\n        temp = np.asmatrix(reshaped_x_grey[i, :, :, :])\n        svd_feat.append(scipy.linalg.svdvals(temp))\n\n    # Compute RGB channel means\n    r_mean = []\n    g_mean = []\n    b_mean = []\n\n    for i in range(len(reshaped_x_new)):\n        r_mean.append(np.mean(reshaped_x_new[i, :, :, 0]))\n        g_mean.append(np.mean(reshaped_x_new[i, :, :, 1]))\n        b_mean.append(np.mean(reshaped_x_new[i, :, :, 2]))\n\n    df_means = pd.DataFrame(list(zip(r_mean, g_mean, b_mean)), columns=['R Mean', 'G Mean', 'B Mean'])\n    svd_feat_np = np.asarray(svd_feat)\n    df_svd_feat = pd.DataFrame(svd_feat_np)\n\n    # Combine SVD features and channel means\n    df_X = pd.concat([df_svd_feat, df_means], axis=1)\n    df_X.columns = df_X.columns.astype(str)\n    initial_len = len(df_X)\n    df_X = df_X.dropna()\n    return df_X\n\ndef image_processing(df_x):\n    reshaped_X = df_x.values.reshape(-1, 28, 28, 4).astype(float)\n    reshaped_X_new = reshaped_X / 255.0\n    reshaped_X_rgb = reshaped_X_new[:, :, :, :3]\n    return reshaped_X_rgb\n\ndef label_processing(df_y):\n    df_y['Labels'] = \"NA\"\n    for ix in range(len(df_y)):\n        if df_y.iloc[ix, 0] == 1:\n            df_y.iloc[ix, 4] = \"Barren Land\"\n        elif df_y.iloc[ix, 1] == 1:\n            df_y.iloc[ix, 4] = \"Trees\"\n        elif df_y.iloc[ix, 2] == 1:\n            df_y.iloc[ix, 4] = \"Grassland\"\n        else:\n            df_y.iloc[ix, 4] = \"None\"\n    df_y = df_y['Labels']\n    label_map = {\"Barren Land\": 0, \"Trees\": 1, \"Grassland\": 2, \"None\": 3}\n    labels = df_y.map(label_map).values\n    return labels\n\ndef train_and_evaluate_model(model, train_loader, test_loader, num_epochs=10):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    start_time = time.time()\n    process = psutil.Process()\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n    \n    end_time = time.time()\n    time_taken = end_time - start_time\n    memory_info = process.memory_info()\n    cpu_times = process.cpu_times()\n    \n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)  # Move images and labels to GPU\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n\n    return {\n        \"time_taken\": time_taken,\n        \"memory_usage\": memory_info.rss / (1024 ** 2),\n        \"cpu_time\": cpu_times.user + cpu_times.system,\n        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n        \"classification_report\": classification_report(y_true, y_pred, output_dict=True)\n    }\n\ndef train_and_evaluate_classifiers(X_train, y_train, X_test, y_test):\n    classifiers = {\n        'Logistic Regression': LogisticRegression(),\n        'Decision Tree': DecisionTreeClassifier(),\n        'K-Nearest Neighbors': KNeighborsClassifier(),\n        'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n        'Gaussian Naive Bayes': GaussianNB(),\n        'Support Vector Machine': SVC(),\n        'Random Forest': RandomForestClassifier(n_estimators=50)\n    }\n\n    start_time = time.time()\n    process = psutil.Process()\n    \n    best_model = None\n    best_score = 0\n    best_model_name = \"\"\n    for name, clf in classifiers.items():\n        clf.fit(X_train, y_train)\n        test_score = clf.score(X_test, y_test)\n        if test_score > best_score:\n            best_score = test_score\n            best_model = clf\n            best_model_name = name\n    \n    end_time = time.time()\n    time_taken = end_time - start_time\n    memory_info = process.memory_info()\n    cpu_times = process.cpu_times()\n    \n    y_pred = best_model.predict(X_test)\n    \n    return {\n        \"time_taken\": time_taken,\n        \"memory_usage\": memory_info.rss / (1024 ** 2),\n        \"cpu_time\": cpu_times.user + cpu_times.system,\n        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n        \"classification_report\": classification_report(y_test, y_pred, output_dict=True),\n        \"best_model_name\": best_model_name,\n        \"best_model_score\": best_score\n    }\n\n# Function to aggregate results\ndef aggregate_results(results_list):\n    aggregated = {\n        \"time_taken\": sum([r['time_taken'] for r in results_list]),\n        \"memory_usage\": sum([r['memory_usage'] for r in results_list]),\n        \"cpu_time\": sum([r['cpu_time'] for r in results_list]),\n        \"confusion_matrix\": np.sum([r['confusion_matrix'] for r in results_list], axis=0),\n        \"classification_report\": {}\n    }\n    \n    for key in results_list[0]['classification_report']:\n        aggregated['classification_report'][key] = {}\n        for metric in results_list[0]['classification_report'][key]:\n            aggregated['classification_report'][key][metric] = np.mean([r['classification_report'][key][metric] for r in results_list])\n\n    return aggregated\n\n# Paths to data chunks\nchunk_train_files = [f\"/kaggle/input/deepsat4-subsets/chunk_x_train_{i}.csv\" for i in range(1, 9)]\nchunk_train_labels = [f\"/kaggle/input/deepsat4-subsets/chunk_y_train_{i}.csv\" for i in range(1, 9)]\nchunk_test_files = [f\"/kaggle/input/deepsat4-subsets/chunk_x_test_{i}.csv\" for i in range(1, 9)]\nchunk_test_labels = [f\"/kaggle/input/deepsat4-subsets/chunk_y_test_{i}.csv\" for i in range(1, 9)]\n\n# Aggregated results\nresults_regular_list = []\nresults_vit_list = []\nresults_resnet_list = []\n\n# Process each chunk\nfor train_file, train_label_file, test_file, test_label_file in zip(chunk_train_files, chunk_train_labels, chunk_test_files, chunk_test_labels):\n    df_x_train = pd.read_csv(train_file)\n    df_y_train = pd.read_csv(train_label_file)\n    df_x_test = pd.read_csv(test_file)\n    df_y_test = pd.read_csv(test_label_file)\n\n    train_labels = label_processing(df_y_train)\n    test_labels = label_processing(df_y_test)\n\n    # Data preprocessing for regular classifiers\n    reshaped_x_train_regular = image_processing_regular_classification(df_x_train)\n    reshaped_x_test_regular = image_processing_regular_classification(df_x_test)\n\n    # Data preprocessing for ViT and ResNet\n    reshaped_x_train_rgb = image_processing(df_x_train)\n    reshaped_x_test_rgb = image_processing(df_x_test)\n\n    train_dataset = Sat4Dataset(reshaped_x_train_rgb, train_labels, transform)\n    test_dataset = Sat4Dataset(reshaped_x_test_rgb, test_labels, transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n    # Evaluate regular classifiers\n    results_regular = train_and_evaluate_classifiers(reshaped_x_train_regular, train_labels, reshaped_x_test_regular, test_labels)\n    results_regular_list.append(results_regular)\n\n    # Evaluate ViT model\n    vit_model = ViTModel().to(device)\n    results_vit = train_and_evaluate_model(vit_model, train_loader, test_loader)\n    results_vit_list.append(results_vit)\n\n    # Evaluate ResNet model\n    resnet_model = ResNetModel().to(device)\n    results_resnet = train_and_evaluate_model(resnet_model, train_loader, test_loader)\n    results_resnet_list.append(results_resnet)\n\n# Aggregate results\naggregated_results_regular = aggregate_results(results_regular_list)\naggregated_results_vit = aggregate_results(results_vit_list)\naggregated_results_resnet = aggregate_results(results_resnet_list)\n\n# Print aggregated results\nprint(\"Aggregated Regular Classifiers:\")\nprint(f\"Time taken: {aggregated_results_regular['time_taken']:.2f} seconds\")\nprint(f\"Memory usage: {aggregated_results_regular['memory_usage']:.2f} MB\")\nprint(f\"CPU time: {aggregated_results_regular['cpu_time']:.2f} seconds\")\nprint(\"Confusion Matrix:\")\nprint(aggregated_results_regular['confusion_matrix'])\nprint(\"Classification Report:\")\nprint(pd.DataFrame(aggregated_results_regular['classification_report']).transpose())\n\nprint(\"\\nAggregated ViT Model:\")\nprint(f\"Time taken: {aggregated_results_vit['time_taken']:.2f} seconds\")\nprint(f\"Memory usage: {aggregated_results_vit['memory_usage']:.2f} MB\")\nprint(f\"CPU time: {aggregated_results_vit['cpu_time']:.2f} seconds\")\nprint(\"Confusion Matrix:\")\nprint(aggregated_results_vit['confusion_matrix'])\nprint(\"Classification Report:\")\nprint(pd.DataFrame(aggregated_results_vit['classification_report']).transpose())\n\nprint(\"\\nAggregated ResNet Model:\")\nprint(f\"Time taken: {aggregated_results_resnet['time_taken']:.2f} seconds\")\nprint(f\"Memory usage: {aggregated_results_resnet['memory_usage']:.2f} MB\")\nprint(f\"CPU time: {aggregated_results_resnet['cpu_time']:.2f} seconds\")\nprint(\"Confusion Matrix:\")\nprint(aggregated_results_resnet['confusion_matrix'])\nprint(\"Classification Report:\")\nprint(pd.DataFrame(aggregated_results_resnet['classification_report']).transpose())\n","metadata":{},"execution_count":null,"outputs":[]}]}